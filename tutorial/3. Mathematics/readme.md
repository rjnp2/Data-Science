# The Mathematics of Machine Learning

Machine Learning theory is a field that intersects statistical, probabilistic, computer science and algorithmic aspects arising from learning iteratively from data and finding hidden insights which can be used to build intelligent applications.

![105940599-c9f21580-6083-11eb-8754-1322d5eaa4ad](https://user-images.githubusercontent.com/58425689/107522138-7b905b00-6bdb-11eb-9e3b-3dfcad4ca486.png)
![105940669-eee68880-6083-11eb-9f18-caa1d33f07dd](https://user-images.githubusercontent.com/58425689/107522146-7d5a1e80-6bdb-11eb-8a1c-8c3ba64d9091.png)

## 1. Linear Algebra:
  - Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms.
  - We represent numerical data as vectors and represent a table of such data as a matrix. The study of vectors and matrices is called linear algebra.
  - It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code.  
  - Topics such as
       - [**Simple_Algebra**](https://github.com/rjnp2/Data-Science/blob/main/tutorial/3.%20Mathematics/1.%20linear_algebra/1.%20algebra.md), [**Vectors_Spaces**](https://github.com/rjnp2/Data-Science/blob/main/tutorial/3.%20Mathematics/1.%20linear_algebra/2.%20vector.md) and [**Norms**](https://github.com/rjnp2/Data-Science/blob/main/tutorial/3.%20Mathematics/1.%20linear_algebra/3.%20vector_norm.md) 
       - [**Matrices and it's Operations, Matrix Decomposition/Factorization, Singular Value Decomposition (SVD),Orthogonalization & Orthonormalization**](https://github.com/rjnp2/Data-Science/blob/main/tutorial/3.%20Mathematics/1.%20linear_algebra/4.%20Matrices%20and%20Vectors.md)
       - [**Eigenvalues & Eigenvectors, Eigendecomposition of a matrix , Principal Component Analysis (PCA)**](https://github.com/rjnp2/Data-Science/blob/main/tutorial/3.%20Mathematics/1.%20linear_algebra/5.%20Eigenvalues%20%26%20Eigenvectors.md)
       - **Projections.**
  
## 2. Probability Theory and Statistics: 
  - Machine Learning and Statistics aren’t very different fields.
  - Some of the fundamental Statistical and Probability Theory needed for ML are 
      
      - **Counting Techniques**
      
      - **Probability Rules & Axioms, Bayes’ Theorem, Random Variables,Variance and Expectation Conditional and Joint Distributions, Standard Distributions (Bernoulli, Binomial, Multinomial, Uniform and Gaussian**
      - **Statistics - Descriptive & Inferential**
      - **Moment Generating Functions, Maximum Likelihood Estimation (MLE), Prior and Posterior, Maximum a Posteriori Estimation (MAP).**

## 3. Multivariate Calculus: 
  - Some of the necessary topics include 
    
      - **Differential and Integral Calculus, Partial Derivatives, Vector-Values Functions, Directional Gradient** 
      
      - **Hessian, Jacobian, Laplacian and Lagragian Distribution.**
  
## 4. Algorithms and Complex Optimizations: 
  - This is important for understanding the computational efficiency and scalability of our Machine Learning Algorithm and for exploiting sparsity in our            datasets.
  - Knowledge of
  
      - **data structures (Binary Trees, Hashing, Heap, Stack etc),Randomized & Sublinear Algorithm, Graphs, Gradient/Stochastic Descents and Primal-Dual methods**

## 5. Others: 
  - This comprises of other Math topics not covered in the four major areas described above. 
  - They include 
  
      - **Real and Complex Analysis (Sets and Sequences, Topology, Metric Spaces, Cauchy Kernel, Fourier Transforms)** 
      
      - **Information Theory (Entropy, Information Gain)** 
      - **Function Spaces and Manifolds.**
