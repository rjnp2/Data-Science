# The Mathematics of Machine Learning

Machine Learning theory is a field that intersects statistical, probabilistic, computer science and algorithmic aspects arising from learning iteratively from data and finding hidden insights which can be used to build intelligent applications.

## Why Worry About The Maths?
There are many reasons why the mathematics of Machine Learning is important and I will highlight some of them below:
1. Selecting the right algorithm which includes giving considerations to accuracy, training time, model complexity, number of parameters and number of features.
2. Choosing parameter settings and validation strategies.
3. Identifying underfitting and overfitting by understanding the Bias-Variance tradeoff.
4. Estimating the right confidence interval and uncertainty.

![image](https://user-images.githubusercontent.com/58425689/105940669-eee68880-6083-11eb-9f18-caa1d33f07dd.png)
![image](https://user-images.githubusercontent.com/58425689/105940599-c9f21580-6083-11eb-8754-1322d5eaa4ad.png)

## 1. Linear Algebra: 
  - We represent numerical data as vectors and represent a table of such data as a matrix. The study of vectors and matrices is called linear algebra.
  - Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms.
  - It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code.
  - Although linear algebra is integral to the field of machine learning, the tight relationship is often left unexplained or explained using abstract concepts such as vector spaces or specific matrix operations.
  - Topics such as
  
       - **Vectors Spaces and Norms are needed for understanding the optimization methods used for machine learning,** 
       - **Linear Transformation And Matrices, Matrix Operations , Symmetric Matrices,LU Decomposition, QR Decomposition/Factorization**
       - **Eigenvalues & Eigenvectors, Eigendecomposition of a matrix**
       - **Orthogonalization & Orthonormalization**
       - **Principal Component Analysis (PCA), Singular Value Decomposition (SVD)** 
       - **Projections**
  
## 2. Probability Theory and Statistics: 
  - Machine Learning and Statistics aren’t very different fields.
  - Some of the fundamental Statistical and Probability Theory needed for ML are **Combinatorics, Probability Rules & Axioms, Bayes’ Theorem, Random Variables,         Variance and Expectation, Conditional and Joint Distributions, Standard Distributions (Bernoulli, Binomial, Multinomial, Uniform and Gaussian), Moment             Generating Functions, Maximum Likelihood Estimation (MLE), Prior and Posterior, Maximum a Posteriori Estimation (MAP) and Sampling Methods.**

## 3. Multivariate Calculus: 
  - Some of the necessary topics include **Differential and Integral Calculus, Partial Derivatives, Vector-Values Functions, Directional Gradient, Hessian, Jacobian, Laplacian and Lagragian Distribution.**
  
## 4. Algorithms and Complex Optimizations: 
  - This is important for understanding the computational efficiency and scalability of our Machine Learning Algorithm and for exploiting sparsity in our            datasets.
  - Knowledge of **data structures (Binary Trees, Hashing, Heap, Stack etc), Dynamic Programming, Randomized & Sublinear Algorithm, Graphs, Gradient/Stochastic Descents and Primal-Dual methods are needed.**

## 5. Others: 
  - This comprises of other Math topics not covered in the four major areas described above. 
  - They include **Real and Complex Analysis (Sets and Sequences, Topology, Metric Spaces, Single-Valued and Continuous Functions, Limits, Cauchy Kernel, Fourier Transforms), Information Theory (Entropy, Information Gain), Function Spaces and Manifolds.**
