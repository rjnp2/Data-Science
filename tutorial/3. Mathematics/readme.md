# The Mathematics of Machine Learning

Machine Learning theory is a field that intersects statistical, probabilistic, computer science and algorithmic aspects arising from learning iteratively from data and finding hidden insights which can be used to build intelligent applications.
![105940599-c9f21580-6083-11eb-8754-1322d5eaa4ad](https://user-images.githubusercontent.com/58425689/107521567-d07fa180-6bda-11eb-8a55-f6d0534768c4.png)
![105940669-eee68880-6083-11eb-9f18-caa1d33f07dd](https://user-images.githubusercontent.com/58425689/107521578-d2496500-6bda-11eb-9968-49712c34db71.png)

## 1. Linear Algebra: 
  - We represent numerical data as vectors and represent a table of such data as a matrix. The study of vectors and matrices is called linear algebra.
  - Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms.
  - It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code.
  - Although linear algebra is integral to the field of machine learning, the tight relationship is often left unexplained or explained using abstract concepts such as vector spaces or specific matrix operations.
  - Topics such as
  
       - **Vectors Spaces and Norms are needed for understanding the optimization methods used for machine learning.** 
       
       - **Matrices, Matrix Operations , Symmetric Matrices,LU Decomposition, QR Decomposition/Factorization ,Orthogonalization & Orthonormalization**
       - **Eigenvalues & Eigenvectors, Eigendecomposition of a matrix.**
       - **Principal Component Analysis (PCA), Singular Value Decomposition (SVD).** 
       - **Projections.**
  
## 2. Probability Theory and Statistics: 
  - Machine Learning and Statistics aren’t very different fields.
  - Some of the fundamental Statistical and Probability Theory needed for ML are 
      
      - **Counting Techniques**
      
      - **Probability Rules & Axioms, Bayes’ Theorem, Random Variables,Variance and Expectation Conditional and Joint Distributions, Standard Distributions (Bernoulli, Binomial, Multinomial, Uniform and Gaussian**
      - **Statistics - Descriptive & Inferential**
      - **Moment Generating Functions, Maximum Likelihood Estimation (MLE), Prior and Posterior, Maximum a Posteriori Estimation (MAP).**

## 3. Multivariate Calculus: 
  - Some of the necessary topics include 
    
      - **Differential and Integral Calculus, Partial Derivatives, Vector-Values Functions, Directional Gradient** 
      
      - **Hessian, Jacobian, Laplacian and Lagragian Distribution.**
  
## 4. Algorithms and Complex Optimizations: 
  - This is important for understanding the computational efficiency and scalability of our Machine Learning Algorithm and for exploiting sparsity in our            datasets.
  - Knowledge of
  
      - **data structures (Binary Trees, Hashing, Heap, Stack etc),Randomized & Sublinear Algorithm, Graphs, Gradient/Stochastic Descents and Primal-Dual methods**

## 5. Others: 
  - This comprises of other Math topics not covered in the four major areas described above. 
  - They include 
  
      - **Real and Complex Analysis (Sets and Sequences, Topology, Metric Spaces, Cauchy Kernel, Fourier Transforms)** 
      
      - **Information Theory (Entropy, Information Gain)** 
      - **Function Spaces and Manifolds.**
