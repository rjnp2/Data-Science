# Information Theory
 - Use in Decision Tree
 
## Gini-Index
- Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen.
- The Gini Index measures the impurity of data set (D) as: \
          ![image](https://user-images.githubusercontent.com/58425689/107509712-074dbb80-6bcb-11eb-8550-055a5c05be50.png)

                      Where, n = Number of classes,
                             Pi= Probability of i th class.
- It consider binary split for each attribute.
- When D is partition into D 1 and D 2 then, \
    Gini(D) = D 1 /D Gini(D 1 ) + D 2 /D Gini(D 2 )
- The attribute that maximize the reduction in impurity is selected as splitting attribute.

## Entropy
- Entropy H(S) is a measure of the amount of uncertainty in the dataset (S) (i.e. entropy characterizes the dataset (S)).
Where, \
![Screenshot_20210210-175734_Adobe Acrobat](https://user-images.githubusercontent.com/58425689/107510345-e33eaa00-6bcb-11eb-878b-4e111123f5cf.jpg)
                     
                     Where, S = The current dataset for which entropy is being calculated
                            (changes every iteration of the ID3 algorithm),
                            X = Set of classes in S, 
                            P(x) - The probability of each set S
                            
## Information Gain
- Information gain is the measure of the difference in entropy from before to after the set S is split on an attribute A. In other words, how much uncertainty in dataset (S) was reduced after splitting dataset S on attribute A. \
    ![image](https://user-images.githubusercontent.com/58425689/107510746-8db6cd00-6bcc-11eb-8170-a29aa984593a.png)

                      Where, H(S) = Entropy of dataset S,
                      T - The subsets created from splitting dataset S by attribute A,
                      P(t) - The probability of class t,
                      H(t) - Entropy of subset t
                      
- In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the largest information gain is used to split the set S on this iteration.
