{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "clear-texas",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Let's get started !!!\n",
    "### 1. Importing the required libraries for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np                     # For mathematical calculations \n",
    "import seaborn as sns                  # For data visualization \n",
    "import matplotlib.pyplot as plt        # For plotting graphs \n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "import warnings                        # To ignore any warnings warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-teens",
   "metadata": {},
   "source": [
    "## 2. Loading the data into the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"example.\",delimiter=' ')\n",
    "# To display the top 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-battle",
   "metadata": {},
   "source": [
    "###  Firstly, we will check the features present in our data and then we will look at their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-cement",
   "metadata": {},
   "source": [
    "#### Given below is the description for each variable.\n",
    "\n",
    "| Variable | Description | \n",
    "|---|---|\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-breath",
   "metadata": {},
   "source": [
    "### 3. Checking the types of data\n",
    "\n",
    "Here we check for the datatypes because sometimes ***variable** would be stored as a string or object, if in that case, we have to convert that string to the integer data only then we can plot the data via a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-baker",
   "metadata": {},
   "source": [
    "### We can see there are three format of data types:\n",
    "1. object: descrip\n",
    "2. int64: descrip\n",
    "2. float64: descrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-economy",
   "metadata": {},
   "source": [
    "### 4. Dropping irrelevant columns\n",
    "\n",
    "This step is certainly needed in every EDA because sometimes there would be many columns that we never use in such cases dropping is the only solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "df = df.drop([ #column_names \n",
    "            ])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-finish",
   "metadata": {},
   "source": [
    "### 5. Renaming the columns\n",
    "\n",
    "In this instance, most of the column names are very confusing to read, so I just tweaked their column names. This is a good approach it improves the readability of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names\n",
    "df = df.rename(columns={}\n",
    "              )\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-brook",
   "metadata": {},
   "source": [
    "### 6. Dropping the duplicate rows\n",
    "\n",
    "This is often a handy thing to do because a huge data set as often have some duplicate data which might be disturbing, so here I remove all the duplicate value from the data-set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of rows and columns\n",
    "print(df.shape)\n",
    "\n",
    "# Rows containing duplicate data\n",
    "duplicate_rows_df = df[df.duplicated()]\n",
    "print(\"number of duplicate rows: \", duplicate_rows_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicates \n",
    "df = df.drop_duplicates()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-bouquet",
   "metadata": {},
   "source": [
    "***Here, notices mean, medain(represented by 50%(50th percentile)) value of each column in index column. \\\n",
    "      notices difference between 75th %tile and max values of predictors \\\n",
    "      observed any extreme values-Outliers in our data set.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-jerusalem",
   "metadata": {},
   "source": [
    "## Next Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-genetics",
   "metadata": {},
   "source": [
    "## 7. Univariate Analysis\n",
    "\n",
    "**In this section, we will do univariate analysis. It is the simplest form of analyzing data where we examine each variable individually. For categorical features we can use frequency table or bar plots which will calculate the number of each category in a particular variable. For numerical features, probability density plots can be used to look at the distribution of the variable.**\n",
    "\n",
    "### Target Variable( if categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first look at the target variable, i.e., Loan_Status. \n",
    "# As it is a categorical variable, let us look at its frequency table, percentage distribution and bar plot.\n",
    "\n",
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize can be set to True to print proportions instead of number \n",
    "df['Target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-bailey",
   "metadata": {},
   "source": [
    "### Target Variable( if Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(121) \n",
    "sns.distplot(df['Target']); \n",
    "plt.subplot(122)\n",
    "df['Target'].plot.box(figsize=(16,5)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-australia",
   "metadata": {},
   "source": [
    "### Independent Variable (if Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1) \n",
    "\n",
    "plt.subplot(221)\n",
    "df['Variable'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Variable name') \n",
    "plt.subplot(222)\n",
    "df['Variable'].value_counts(normalize=True).plot.bar(title= 'Variable name') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-tobago",
   "metadata": {},
   "source": [
    "### It can be inferred from the above bar plots that:\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-peter",
   "metadata": {},
   "source": [
    "### Independent Variable (if Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1) \n",
    "\n",
    "plt.subplot(221)\n",
    "df['Variable'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Variable name') \n",
    "plt.subplot(222)\n",
    "df['Variable'].value_counts(normalize=True).plot.bar(title= 'Variable name') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-candle",
   "metadata": {},
   "source": [
    "### It can be inferred from the above bar plots that:\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-climate",
   "metadata": {},
   "source": [
    "### Independent Variable (if Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(121) \n",
    "sns.distplot(train['Variable']); \n",
    "plt.subplot(122)\n",
    "train['Variable'].plot.box(figsize=(16,5)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-bicycle",
   "metadata": {},
   "source": [
    "### Following inferences can be made from the above dist  plots:\n",
    "\n",
    "1. Most of the data in the distribution of is towards right / left which means it is not normally distributed. We will try to make it normal in later sections as algorithms works better if the data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-opposition",
   "metadata": {},
   "source": [
    "### After looking at every variable individually in univariate analysis, we will now explore them again with respect to the target variable(if Categorical).\n",
    "\n",
    "## 8. Univariate Analysis(if target variable is Categorical)\n",
    "\n",
    "### 8.1 Categorical Independent Variable vs Target Variable\n",
    "First of all we will find the relation between target variable and categorical independent variables. Let us look at the stacked bar plot now which will give us the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender=pd.crosstab(train['Categorical'],train['Target']) \n",
    "Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-fight",
   "metadata": {},
   "source": [
    "### Following inferences can be made from the above proportion plots:\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-stations",
   "metadata": {},
   "source": [
    "### 8.2 Numerical Independent Variable vs Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=[] # bin range\n",
    "\n",
    "group=[] # group name\n",
    "\n",
    "train['extra_col']=pd.cut(train['variable'],bins,labels=group)\n",
    "\n",
    "Income_bin=pd.crosstab(train['extra_col'],train['variable']) \n",
    "\n",
    "Income_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \n",
    "\n",
    "plt.xlabel('') \n",
    "P = plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-things",
   "metadata": {},
   "source": [
    "### Following inferences can be made from the above bar plots:\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-minimum",
   "metadata": {},
   "source": [
    "## 9. Univariate Analysis(if target variable is Numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-synthetic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-intersection",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-junction",
   "metadata": {},
   "source": [
    "### Now lets look at the correlation between all the numerical variables.\n",
    "### We will use the heat map to visualize the correlation. Heatmaps visualize data through variations in coloring. \n",
    "### The variables with darker color means their correlation is more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the relations between the variables.\n",
    "plt.figure(figsize=(20,10))\n",
    "matrix = train.corr()\n",
    "sns.heatmap(matrix, vmax=.8, square=True, cmap=\"BuPu\",annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-reservation",
   "metadata": {},
   "source": [
    "### To check distribution-Skewness\n",
    "sns.distplot(df[variable],kde=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-difficulty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "piano-adolescent",
   "metadata": {},
   "source": [
    "### 10. irrelevant columns which do not have any effect on the target variable\n",
    "\n",
    "### After exploring all the variables in our data, Lets drop some variable as it do not have any effect on the target variable. \n",
    "### We will do the same changes to the test dataset which we did for the training dataset.\n",
    "#### df=df.drop(['col_name'],axis=1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-benjamin",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-cleaning",
   "metadata": {},
   "source": [
    "# Next Section\n",
    "\n",
    "### 11. Dropping or Replacing the missing or null values.\n",
    "\n",
    "This is mostly similar to the previous step but in here all the missing values are detected and are dropped/repalced later.\\\n",
    "**Dropping: this is not a good approach to do so, but it is done when there is is a small number and this is negligible.\\\n",
    "Replacing: Just replace the missing values with the mean or the median of that column.**\n",
    "\n",
    "After exploring all the variables in our data, we can now impute the missing values and treat the outliers because missing data and outliers can have adverse effect on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the null values.\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-tender",
   "metadata": {},
   "source": [
    "We can consider these methods to fill the missing values:\n",
    "\n",
    "***1. For numerical variables: imputation using mean or median***\\\n",
    "***2. For categorical variables: imputation using mode***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values with median.\n",
    "\n",
    "df['categorical variables'].fillna(df['categorical variables'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numerical variables'].fillna(df['numerical variables'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-memphis",
   "metadata": {},
   "source": [
    "### 12. Outlier Treatment\n",
    "An outlier is a point or set of points that are different from other points. Sometimes they can be very high or very low. It’s often a good idea to detect and remove the outliers. Because outliers are one of the primary reasons for resulting in a less accurate model. Hence it’s a good idea to remove them. The outlier detection and removing that I am going to perform is called IQR score technique. Often outliers can be seen with visualizations using a box plot.\n",
    "\n",
    "As we saw earlier in univariate analysis, varaible can contains outliers so we have to treat them as the presence of outliers affects the distribution of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['var_log'] = np.log(df['var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-musician",
   "metadata": {},
   "source": [
    "### 13.Get_Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-invention",
   "metadata": {},
   "source": [
    "# Next Section\n",
    "\n",
    "### 14. Feature Engineering\n",
    "\n",
    "Based on the domain knowledge, we can come up with new features that might affect the target variable. We will create the following three new features:\n",
    "\n",
    "BY: adding/subtracting two columns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-community",
   "metadata": {},
   "source": [
    "# Last Section\n",
    "\n",
    "save csv  \n",
    "df.to_csv('data_final.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
