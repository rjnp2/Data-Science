{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "absent-novelty",
   "metadata": {},
   "source": [
    "# Feeding Data to the Training Algorithm\n",
    "\n",
    "To implement Mini-batch Gradient Descent, we only need to tweak the existing code\n",
    "slightly.\n",
    "\n",
    "First change the definition of X and y in the construction phase to make them\n",
    "placeholder nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import matplotlib.pyplot as plt   \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "train_images = train_images.reshape(-1,784)\n",
    "train_labels = train_labels.reshape(-1,1)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images= train_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "train_labels = enc.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters   \n",
    "learning_rate = 0.01   \n",
    "training_epochs = 25   \n",
    "batch_size = 100   \n",
    "display_step = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input   \n",
    "x = tf.placeholder(\"float\", [None, 784]) # MNIST data image of shape 28*28 = 784   \n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model   \n",
    "# Set model weights   \n",
    "W = tf.Variable(tf.zeros([784, 10]))   \n",
    "b = tf.Variable(tf.zeros([10]))   \n",
    "\n",
    "# Constructing the model  \n",
    "activation=tf.nn.softmax(tf.matmul(x, W)+b) # Softmax of function   \n",
    "    \n",
    "# Minimizing error using cross entropy  \n",
    "cross_entropy = y*tf.log(activation)   \n",
    "cost = tf.reduce_mean(-tf.reduce_sum(cross_entropy))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)  \n",
    "\n",
    "#Plot settings   \n",
    "avg_set = []   \n",
    "epoch_set = []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(X,y,batch_size=64):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    for i in np.arange(0, n_samples, batch_size):\n",
    "        begin, end = i, min(i+batch_size, n_samples)\n",
    "        yield X[begin:end], y[begin:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launching the graph  \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  \n",
    "    # Training of the cycle in  the dataset  \n",
    "    for epoch in range(training_epochs):  \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(train_images.shape[0]/batch_size)  \n",
    "        \n",
    "        # Creating loops at all the batches in the code \n",
    "        for batch_xs, batch_ys in batch_iterator(train_images, train_labels,batch_size) :  \n",
    "            # Fitting the training by the batch data \n",
    "            sess.run(activation,  feed_dict = {x: batch_xs, y: batch_ys})  \n",
    "            # Compute all the average of loss\n",
    "            avg_cost += sess.run(cost, feed_dict = { x: batch_xs, y: batch_ys}) \n",
    "            # Display the logs at each epoch steps   \n",
    "            if epoch % display_step==0:   \n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format (avg_cost))  \n",
    "            avg_set.append(avg_cost)\n",
    "            epoch_set.append(epoch+1)  \n",
    "    print (\"Training phase finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-terrace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
